{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9d7ec4-1f98-4f48-ace5-e504edc2a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "#device = \"cpu\"\n",
    "#print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2c9ba4-c0c6-4baf-876b-7b455e4a6997",
   "metadata": {},
   "outputs": [],
   "source": [
    "saiga_yandexgpt_8b = \"C:/Users/KseniaLuschevaExt/Documents/models/saiga_yandexgpt_8b\"\n",
    "rut5_base_headline_gen_telegram = \"C:/Users/KseniaLuschevaExt/Documents/models/rut5_base_headline_gen_telegram\"\n",
    "rugpt3large_based_on_gpt2 = \"C:/Users/KseniaLuschevaExt/Documents/models/rugpt3large_based_on_gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa67f29c-524b-4201-a8e8-1610b6b2153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_POOL = [\n",
    "    {\n",
    "        \"name\": saiga_yandexgpt_8b,\n",
    "        \"type\": \"chat\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"model_class\": \"causal\",\n",
    "        \"device\": \"cpu\"\n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e39897b-3c74-4999-af58-2c81deaf4d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,  # For GPT-like models\n",
    "    AutoModelForSeq2SeqLM,  # For T5-like models\n",
    "    AutoTokenizer,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "\n",
    "class ModelWrapper:\n",
    "    def __init__(self, model_info):\n",
    "        self.device = \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_info[\"name\"])\n",
    "        \n",
    "        # Get config first to determine architecture\n",
    "        config = AutoConfig.from_pretrained(model_info[\"name\"])\n",
    "        \n",
    "        # Select appropriate model class\n",
    "        if model_info[\"model_class\"] == \"causal\":\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_info[\"name\"],\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        elif model_info[\"model_class\"] == \"seq2seq\":\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                model_info[\"name\"],\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model class: {model_info['model_class']}\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        self.model_type = model_info[\"type\"]\n",
    "        self.default_params = {\n",
    "            \"max_new_tokens\":500,\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\": 0.9,\n",
    "            \"temperature\": model_info[\"temperature\"]\n",
    "        }\n",
    "\n",
    "    def generate(self, prompt, **kwargs):\n",
    "        try:\n",
    "            # Merge default params with any overrides\n",
    "            params = {**self.default_params, **kwargs}\n",
    "            \n",
    "            if isinstance(self.model, AutoModelForSeq2SeqLM):\n",
    "                # T5-style models\n",
    "                inputs = self.tokenizer(\n",
    "                    prompt, \n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                ).to(self.device)\n",
    "                \n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    **params\n",
    "                )\n",
    "            else:  # Causal LM\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    **params,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {self.model.config.name_or_path}: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cec5003-4af3-4b00-944b-f1109338e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    \"изменение климата\", \"искусственный интеллект\", \"исследование космоса\",  \n",
    "    \"квантовые вычисления\", \"древние цивилизации\", \"технологии будущего\",  \n",
    "    \"нейробиология\", \"криптовалюта\", \"древние мифы\", \"виртуальная реальность\",  \n",
    "    \"синтетическая биология\", \"черные дыры\", \"сознание\", \"постапокалиптические общества\",  \n",
    "    \"океанография\", \"путешествия во времени\", \"лингвистика\", \"нанотехнологии\", \"анализ сновидений\",  \n",
    "    \"межзвездная дипломатия\"  \n",
    "]\n",
    "\n",
    "\n",
    "tones = [\n",
    "    \"нейтральном\", \"восторженном\", \"скептическом\", \"юмористическом\",  \n",
    "    \"аналитическом\", \"саркастичном\", \"меланхоличном\", \"вдохновляющем\", \"драматическом\",  \n",
    "    \"причудливом\", \"мрачном\", \"оптимистичном\", \"пессимистичном\", \"загадочном\",  \n",
    "    \"авторитетном\", \"неформальном\", \"романтическом\"  \n",
    "]\n",
    "\n",
    "actions = [\n",
    "    \"объяснить\", \"описать\", \"обсудить\", \"написать рассказ о\",  \n",
    "    \"проанализировать\", \"сравнить и сопоставить\", \"предсказать будущее\",  \n",
    "    \"критиковать\", \"представить мир, где\", \"подвести итог\",  \n",
    "    \"защитить\", \"оспаривать\", \"переосмыслить\", \"придумать новую теорию о\",  \n",
    "    \"пародировать\", \"взять интервью у эксперта по\", \"написать новостной репортаж о\",  \n",
    "    \"создать диалог о\", \"перечислить плюсы и минусы\", \"исследовать этические аспекты\"  \n",
    "]\n",
    "\n",
    "styles = [\"формальный\", \"деловой\", \"шуточный\", \"школьный\", \"творческий\", \"публицистический\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb7dddc-5eb7-489f-862a-6dc32b9a7b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts(num_prompts=10):\n",
    "    prompts = []\n",
    "    for _ in range(num_prompts):\n",
    "        style = random.choice(styles)\n",
    "        topic = random.choice(topics)\n",
    "        tone = random.choice(tones)\n",
    "        action = random.choice(actions)\n",
    "        \n",
    "        prompt = (\n",
    "            f\"Напиши текст в стиле {style} в тональности '{tone}'. \"\n",
    "            f\"Текст должен {action} тему {topic}'. \"\n",
    "            f\"Убедись, что текст выражает  {style} style appropriately.\"\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d3ff9ce-96ca-41ab-a559-b16e5e75acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from functools import partial\n",
    "\n",
    "def generate_corpus(num_texts, output_csv=\"generated_corpus.csv\", max_workers=4, batch_size=8):\n",
    "    \"\"\"\n",
    "    Generates corpus using diverse prompts and parallel model execution.\n",
    "\n",
    "    Args:\n",
    "        num_texts (int): Total texts to generate\n",
    "        output_csv (str): Output file path\n",
    "        max_workers (int): Parallel threads (match GPU count)\n",
    "        batch_size (int): Texts per parallel batch\n",
    "    \"\"\"\n",
    "    # 1. Initialize models\n",
    "    models = {}\n",
    "    for model_info in MODEL_POOL:\n",
    "        try:\n",
    "            wrapper = ModelWrapper(model_info)\n",
    "            models[model_info[\"name\"]] = wrapper\n",
    "            print(f\"✅ Loaded {model_info['name']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {model_info['name']}: {str(e)}\")\n",
    "\n",
    "    if not models:\n",
    "        raise ValueError(\"No models available!\")\n",
    "\n",
    "    def create_prompt():\n",
    "        style = random.choice(styles)\n",
    "        topic = random.choice(topics)\n",
    "        tone = random.choice(tones)\n",
    "        action = random.choice(actions)\n",
    "\n",
    "        prompt = f\"Ты профессиональный писатель. Требуется {action} '{topic}' в {tone} тоне, используя {style} стиль. Текст должен быть законченным произведением из 300+ слов.\\n\\n Текст должен начинаться сразу с раскрытия темы:\\n\"\n",
    "\n",
    "        full_prompt = f\"\"\"\n",
    "          [ИНСТРУКЦИИ]\n",
    "          {prompt}\n",
    "\n",
    "          [ТРЕБОВАНИЯ]\n",
    "          - Начни текст сразу с содержания\n",
    "          - Не повторяй инструкции\n",
    "          - Сохрани указанный стиль и тон\n",
    "\n",
    "          [НАЧАЛО ТЕКСТА]\n",
    "          \"\"\"\n",
    "\n",
    "        return full_prompt, style\n",
    "\n",
    "    all_prompts, all_styles = zip(*[create_prompt() for _ in range(num_texts)])\n",
    "\n",
    "    def extract_topic(prompt):\n",
    "        \"\"\"Extract main topic from prompt\"\"\"\n",
    "        match = re.search(r\"тему ['\\\"](.+?)['\\\"]\", prompt)\n",
    "        return match.group(1) if match else None\n",
    "\n",
    "    def validate_output(text, prompt):\n",
    "        \"\"\"Enhanced validation with multiple checks\"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return False\n",
    "        \n",
    "        # 1. Length check\n",
    "        word_count = len(text.split())\n",
    "        if word_count < 300:\n",
    "            return False\n",
    "        \n",
    "        # 2. Instruction contamination check\n",
    "        forbidden_phrases = [\n",
    "            \"инструкция\", \"требование\", \"напиши текст\", \n",
    "            \"текст должен\", \"используй стиль\", \"в тоне\"\n",
    "        ]\n",
    "        if any(phrase in text.lower() for phrase in forbidden_phrases):\n",
    "            return False\n",
    "        \n",
    "        # 3. Coherence check (simple version)\n",
    "        sentence_count = len(re.findall(r'[.!?]+', text))\n",
    "        if sentence_count < 5:  # At least 5 sentences\n",
    "            return False\n",
    "        \n",
    "        # 4. Topic relevance (simple version)\n",
    "        topic = extract_topic(prompt)\n",
    "        if topic and topic.lower() not in text.lower():\n",
    "            return False\n",
    "    \n",
    "        return True\n",
    "\n",
    "    def process_batch(batch_prompts, batch_styles):\n",
    "        batch_results = []\n",
    "        for prompt, style in zip(batch_prompts, batch_styles):\n",
    "            model_name = weighted_model_choice(models, style)\n",
    "            model = models[model_name]\n",
    "\n",
    "            try:\n",
    "                # Generation parameters\n",
    "                gen_params = {\n",
    "                    \"temperature\": 0.6,\n",
    "                    \"top_p\": 0.95,\n",
    "                    \"repetition_penalty\": 1.4,\n",
    "                    \"no_repeat_ngram_size\": 3,\n",
    "                    \"max_new_tokens\": 500\n",
    "                }\n",
    "\n",
    "                # Generate text\n",
    "                full_prompt = prompt + \"[НАЧАЛО ТЕКСТА]\\n\"\n",
    "                raw_text = model.generate(full_prompt, **gen_params)\n",
    "                \n",
    "                # Post-process and validate\n",
    "                if raw_text:\n",
    "                    # Extract generated content\n",
    "                    text = raw_text.split(\"[НАЧАЛО ТЕКСТА]\")[-1].strip()\n",
    "                    text = re.split(r\"Инструкция:|Примечание:|Требования:\", text)[0].strip()\n",
    "                    \n",
    "                    # Apply validation\n",
    "                    if validate_output(text, prompt):\n",
    "                        # Additional quality checks\n",
    "                        topic = extract_topic(prompt)  # Implement this based on your prompt structure\n",
    "                        word_count = len(text.split())\n",
    "                        \n",
    "                        batch_results.append({\n",
    "                            \"text\": text,\n",
    "                            \"prompt\": prompt,\n",
    "                            \"style\": style,\n",
    "                            \"topic\": topic,\n",
    "                            \"model\": model_name,\n",
    "                            \"length\": word_count,\n",
    "                            \"valid\": True  # Flag for validation\n",
    "                        })\n",
    "                    else:\n",
    "                        batch_results.append({\n",
    "                            \"text\": text,\n",
    "                            \"prompt\": prompt,\n",
    "                            \"valid\": False,\n",
    "                            \"reason\": \"Failed validation\"\n",
    "                        })\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Generation error: {str(e)}\")\n",
    "                batch_results.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"valid\": False,\n",
    "                    \"reason\": f\"Generation error: {str(e)}\"\n",
    "                })\n",
    "        \n",
    "        return batch_results\n",
    "\n",
    "    corpus = []\n",
    "    with tqdm(total=num_texts, desc=\"Generating corpus\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "            for i in range(0, num_texts, batch_size):\n",
    "                batch_p = all_prompts[i:i + batch_size]\n",
    "                batch_s = all_styles[i:i + batch_size]\n",
    "                futures.append(executor.submit(process_batch, batch_p, batch_s))\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                batch_results = future.result()\n",
    "                corpus.extend(batch_results)\n",
    "                pbar.update(len(batch_results))\n",
    "\n",
    "    # 6. Create DataFrame and ensure all columns exist\n",
    "    df = pd.DataFrame(corpus)\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    if 'length' not in df.columns:\n",
    "        if 'text' in df.columns:\n",
    "            df['length'] = df['text'].str.split().str.len()\n",
    "        else:\n",
    "            raise ValueError(\"Generated corpus is missing both 'length' and 'text' columns\")\n",
    "\n",
    "    # Apply filters\n",
    "    df = df.drop_duplicates(subset=['text'])\n",
    "    df = df[df['length'] >= 150]  # Adjusted from 150 to match earlier check\n",
    "\n",
    "    # Add metadata\n",
    "    df['generation_date'] = pd.Timestamp.now()\n",
    "    df['word_count'] = df['text'].str.split().str.len()  # Exact word count\n",
    "\n",
    "    # Save results\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\nSuccessfully generated {len(df)} texts\")\n",
    "    print(\"Columns available:\", df.columns.tolist())\n",
    "    return df\n",
    "\n",
    "def weighted_model_choice(models, style):\n",
    "    \"\"\"Select model with preference for style matching\"\"\"\n",
    "    style_preference = {\n",
    "        \"formal\": [\"IlyaGusev/saiga_yandexgpt_8b\"],\n",
    "        \"creative\": [\"IlyaGusev/saiga_yandexgpt_8b\"]\n",
    "    }\n",
    "\n",
    "    # Try preferred models first\n",
    "    for model_name in style_preference.get(style, []):\n",
    "        if model_name in models:\n",
    "            return model_name\n",
    "\n",
    "    # Fallback to any available model\n",
    "    return random.choice(list(models.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11018c89-793f-4c9c-8677-a844093fc11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus(df):\n",
    "    \"\"\"Remove low-quality entries\"\"\"\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates(subset=[\"text\"])\n",
    "    \n",
    "    # Filter by length\n",
    "    df = df[df[\"text\"].str.split().str.len() >= 50]\n",
    "    \n",
    "    # Add perplexity filtering if available\n",
    "    if \"perplexity\" in df.columns:\n",
    "        df = df[df[\"perplexity\"] < 100]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2e84bf3-2aa1-4d02-8e32-c46c6a7e9916",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acd21a3f-3966-4b10-8a69-4081e6cc3200",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available = lambda: False\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fe227-b39f-4074-9189-dcff9ac89a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733b726b0193468b9e5411ed1baab6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded C:/Users/KseniaLuschevaExt/Documents/models/saiga_yandexgpt_8b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating corpus:   0%|                                                                         | 0/4 [00:00<?, ?it/s]C:\\Users\\KseniaLuschevaext\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:2208: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "Generating corpus:   0%|                                                                         | 0/4 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in C:/Users/KseniaLuschevaExt/Documents/models/saiga_yandexgpt_8b: Tensor on device cuda:0 is not on the expected device meta!Error in C:/Users/KseniaLuschevaExt/Documents/models/saiga_yandexgpt_8b: Tensor on device cuda:0 is not on the expected device meta!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating corpus:   0%|                                                                         | 0/4 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in C:/Users/KseniaLuschevaExt/Documents/models/saiga_yandexgpt_8b: Tensor on device cuda:0 is not on the expected device meta!\n"
     ]
    }
   ],
   "source": [
    "corpus_df = generate_corpus(\n",
    "    num_texts=4,\n",
    "    batch_size=1  # Texts to generate in paralle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3455da4-3f35-4ed6-a980-3617a61bf390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
