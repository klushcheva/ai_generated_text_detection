{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dfd71dd-0403-46e7-b690-e2b1db08f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31721cb4-3fb4-450d-ba31-654a8c9b9c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('scraping.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f82f323-b906-4519-b4fe-96d09ed7d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    fh = logging.FileHandler('scraping_progress.log')\n",
    "    fh.setLevel(logging.INFO)\n",
    "    \n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "    \n",
    "    # Create formatter and add it to the handlers\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "    \n",
    "    # Add the handlers to the logger\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "440d0465-da32-4ce5-9d5c-6dbd648c439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://proza.ru/texts/list.html\"\n",
    "START_YEAR = 2012\n",
    "END_YEAR = 2025\n",
    "OUTPUT_DIR = \"proza_texts\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "REQUEST_DELAY = 1\n",
    "DAY_DELAY = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "313bbf91-1fa0-4855-9b0c-39f9b74f07b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_leap_year(year):\n",
    "    \"\"\"Check if a year is a leap year\"\"\"\n",
    "    if year % 4 != 0:\n",
    "        return False\n",
    "    elif year % 100 != 0:\n",
    "        return True\n",
    "    else:\n",
    "        return year % 400 == 0\n",
    "\n",
    "def get_days_in_month(year, month):\n",
    "    \"\"\"Return number of days in a month, accounting for leap years\"\"\"\n",
    "    if month == 2:\n",
    "        return 29 if is_leap_year(year) else 28\n",
    "    elif month in [4, 6, 9, 11]:\n",
    "        return 30\n",
    "    else:\n",
    "        return 31\n",
    "\n",
    "def generate_dates():\n",
    "    \"\"\"Generate all dates from START_YEAR to END_YEAR\"\"\"\n",
    "    current_date = datetime(START_YEAR, 1, 1)\n",
    "    end_date = datetime(END_YEAR, 12, 31)\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        yield current_date\n",
    "        current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7250f14a-9cac-4d28-a79a-636018892862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_links(date, topic):\n",
    "    \"\"\"Get all article links for a specific date\"\"\"\n",
    "    url = f\"{BASE_URL}?topic=12&year={date.year}&month={date.month:02d}&day={date.day}\"\n",
    "    logger.info(f\"Going to url {url}...\")\n",
    "    try:\n",
    "        logger.info(f\"Fetching articles for {date.strftime('%Y-%m-%d')} in topic {topic}\")\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find all links that match the pattern /YYYY/MM/DD/ followed by numbers\n",
    "            pattern = re.compile(rf'/{date.year}/(0?{date.month}|{date.month})/(0?{date.day}|{date.day})/\\d+$')\n",
    "            \n",
    "            links = []\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                if pattern.search(a['href']):\n",
    "                    links.append(a['href'])\n",
    "            logger.info(f\"These are the links formed: {links}\")\n",
    "            # Convert relative URLs to absolute\n",
    "            absolute_links = [urljoin(\"https://proza.ru/\", link) for link in links]\n",
    "            time.sleep(2)\n",
    "            logger.info(f\"Found {len(absolute_links)} articles for {date.strftime('%Y-%m-%d')}\")\n",
    "            return absolute_links\n",
    "        \n",
    "        logger.warning(f\"Unexpected status code {response.status_code} for {url}\")\n",
    "        return []\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Error fetching links for topic {topic}, {date}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def extract_article_text(article_url):\n",
    "    \"\"\"Extract text from a single article page\"\"\"\n",
    "    try:\n",
    "        logger.debug(f\"Fetching article: {article_url}\")\n",
    "        response = requests.get(article_url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            text_div = soup.find('div', class_='text')\n",
    "            \n",
    "            if text_div:\n",
    "                logger.debug(f\"Successfully extracted text from {article_url}\")\n",
    "                return text_div.get_text(strip=False)  # Keep original formatting\n",
    "            \n",
    "            logger.warning(f\"No text div found in {article_url}\")\n",
    "            return None\n",
    "        \n",
    "        logger.warning(f\"Unexpected status code {response.status_code} for {article_url}\")\n",
    "        return None\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Error fetching article {article_url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def save_text(article_url, text, date):\n",
    "    \"\"\"Save text to a file with metadata\"\"\"\n",
    "    try:\n",
    "        # Create a filename-safe version of the URL\n",
    "        filename = article_url.split('/')[-1]\n",
    "        \n",
    "        # Store data in a structured format\n",
    "        data = {\n",
    "            'url': article_url,\n",
    "            'date': date.strftime('%Y-%m-%d'),\n",
    "            'text': text,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Create year/month subdirectories\n",
    "        year_dir = os.path.join(OUTPUT_DIR, str(date.year))\n",
    "        month_dir = os.path.join(year_dir, f\"{date.month:02d}\")\n",
    "        os.makedirs(month_dir, exist_ok=True)\n",
    "        \n",
    "        # Save as JSON to preserve metadata and text structure\n",
    "        output_path = os.path.join(month_dir, f\"{filename}.json\")\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        logger.debug(f\"Saved article to {output_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving article {article_url}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b489ee54-f451-4cce-8975-0d7011548bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main scraping function\"\"\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    logger.info(\"Starting scraping process\")\n",
    "    logger.info(f\"Date range: {START_YEAR}-01-01 to {END_YEAR}-12-31\")\n",
    "    logger.info(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    \n",
    "    total_articles = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for date in generate_dates():\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        logger.info(f\"\\nProcessing date: {date_str}\")\n",
    "        \n",
    "        article_links = get_article_links(date, topic=30)\n",
    "        if not article_links:\n",
    "            logger.info(f\"No articles found for {date_str}\")\n",
    "            time.sleep(DAY_DELAY)\n",
    "            continue\n",
    "        \n",
    "        # Process articles with progress bar\n",
    "        for link in tqdm(article_links, desc=f\"Articles for {date_str}\", leave=False):\n",
    "            text = extract_article_text(link)\n",
    "            if text:\n",
    "                save_text(link, text, date)\n",
    "                total_articles += 1\n",
    "            \n",
    "            time.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        logger.info(f\"Finished processing {len(article_links)} articles for {date_str}\")\n",
    "        time.sleep(DAY_DELAY)\n",
    "    \n",
    "    # Final statistics\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logger.info(\"\\nScraping completed!\")\n",
    "    logger.info(f\"Total articles processed: {total_articles}\")\n",
    "    logger.info(f\"Total time elapsed: {elapsed_time/60:.2f} minutes\")\n",
    "    logger.info(f\"Average time per article: {elapsed_time/max(1, total_articles):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1404404-e944-4f92-b7d8-64f64f7207eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    main()\n",
    "except KeyboardInterrupt:\n",
    "    logger.info(\"Scraping interrupted by user\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
