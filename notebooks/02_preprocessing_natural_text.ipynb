{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0829256e-1a1b-4f39-b2a6-abdcb643ab9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\ksenialuschevaext\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.14.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 981.5/981.5 kB 23.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\ksenialuschevaext\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langdetect) (1.17.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (pyproject.toml): started\n",
      "  Building wheel for langdetect (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993362 sha256=3f8bc2760e5a1bb5484b34bca7c1ee3942e13bceee96e8aa240378c0906d8f25\n",
      "  Stored in directory: c:\\users\\ksenialuschevaext\\appdata\\local\\pip\\cache\\wheels\\c1\\67\\88\\e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d06811bd-f62c-482e-b801-ed16d2f00788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import emoji\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84732e37-a85e-453a-bdbe-7e1b20de6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Apply common NLP preprocessing to text\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation (keeping apostrophes for contractions)\n",
    "    text = re.sub(r'[^\\w\\s\\']', '', text)\n",
    "    \n",
    "    # Remove extra whitespace and newlines\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7709efc-91fd-4935-8bab-8f838b23b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_file(json_path, min_words=100):\n",
    "    \"\"\"Process a single JSON file and return cleaned text if valid\"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        text = data.get('text', '')\n",
    "        cleaned = clean_text(text)\n",
    "        \n",
    "        # Check word count\n",
    "        word_count = len(cleaned.split())\n",
    "        if word_count >= min_words:\n",
    "            return cleaned\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {json_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77923f90-530b-488f-948b-ac534bf4c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_russian(text):\n",
    "    try:\n",
    "        return detect(text) == 'ru'\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4758c5b1-6595-436f-b340-14592cf1496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(input_dir, output_csv):\n",
    "    \"\"\"Process all files in directory and save to CSV\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in tqdm(files, desc=\"Processing files\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"file path is: {file_path}\")\n",
    "            if file.endswith('.json'):\n",
    "                print(f\"json found: {file}\")\n",
    "                text = process_json_file(file_path)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if is_russian(text) == False:\n",
    "                print(f\"Skipping text {file}! It is not in russian language\")\n",
    "                continue\n",
    "            \n",
    "            if text:\n",
    "                records.append({'text': text, 'source': file})\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    if records:\n",
    "        df = pd.DataFrame(records)\n",
    "        df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "        print(f\"Saved {len(df)} records to {output_csv}\")\n",
    "    else:\n",
    "        print(\"No valid records found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08032390-c7eb-47b1-a80d-95e907e6be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_directory('/Users/Klushcheva/Documents/thesis/proza', 'processed_proza_texts.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
